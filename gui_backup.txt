"""Streamlit UI for the PDF RAG assistant."""

import json
import os
import subprocess
import sys
import platform
from pathlib import Path
from typing import Any, Dict, List, Optional, Tuple
from uuid import uuid4

import requests
import streamlit as st
from dotenv import load_dotenv
from langchain_core.prompts import ChatPromptTemplate
from langchain_openai import ChatOpenAI, OpenAIEmbeddings

import numpy as np

try:
    import faiss  # type: ignore
except Exception:  # noqa: BLE001
    faiss = None  # type: ignore

SOFT_PID_REGISTRY: dict[str, int] = {}


# -----------------------------------------------------------------------------#
# Paths and configuration
# -----------------------------------------------------------------------------#
PROJECT_ROOT = Path(__file__).resolve().parent
BACKEND_ROOT = PROJECT_ROOT.parent / "backend"
DATA_DIR = BACKEND_ROOT / "data" / "raw"
UPLOADS_ROOT = DATA_DIR / "uploads"
INDEX_ROOT = BACKEND_ROOT / "data" / "index"
TOOLS_DIR = BACKEND_ROOT / "tools"
LOCAL_TEI_ROOT = BACKEND_ROOT / "local-llm" / "Embedding"
MODELS_CONFIG_PATH = LOCAL_TEI_ROOT / "models.json"
TEI_CONTAINER_PREFIX = "tei-"

DEFAULT_EMBED_MODEL = os.getenv("EMBEDDING_MODEL")
DEFAULT_CHAT_MODEL = "gpt-4o-mini"

DEFAULT_CHUNK_SIZE = 1200
DEFAULT_CHUNK_OVERLAP = 200

CHUNK_MODES: Dict[str, str] = {
    "structured": "Structured-chunks",
    "direct": "Direct-chunks",
}
DEFAULT_CHUNK_MODE = "structured"

UPLOAD_ALLOWED_EXTS = {"pdf", "docx", "xlsx"}

OPENAI_EMBED_MODELS = ["text-embedding-3-small", "text-embedding-3-large"]
EMBED_BACKENDS: Dict[str, str] = {
    "openai": "Open AI Chat GPT Embedding",
    "tei": "Local Text-Embeddings-Inference",
}

TEI_MODELS: Dict[str, Dict[str, Any]] = {
    "Alibaba-NLP/gte-multilingual-base": {
        "display": "Alibaba 0.3B",
        "config_key": "Alibaba-NLP-gte-multilingual-base",
        "local_dir": LOCAL_TEI_ROOT / "Alibaba-NLP-gte-multilingual-base",
        "download_script": TOOLS_DIR / "download_gte_multilingual_base_tei.py",
        "required_file": "model.safetensors",
    },
    "sentence-transformers/all-MiniLM-L6-v2": {
        "display": "all-MiniLM-L6-v2 (MiniLM, Q4)",
        "config_key": "sentence-transformers-all-MiniLM-L6-v2",
        "local_dir": LOCAL_TEI_ROOT / "sentence-transformers-all-MiniLM-L6-v2",
        "download_script": TOOLS_DIR / "download_all_minilm_l6_v2_tei.py",
        "required_file": "model.safetensors",
    },
}


def resolve_tei_config_key(model_key: str) -> str:
    """Map UI model keys to the config key expected by launch_tei.py/models.json."""
    config = TEI_MODELS.get(model_key, {})
    config_key = config.get("config_key")
    if isinstance(config_key, str) and config_key:
        return config_key
    # Fallback: replace characters unsupported in JSON config keys.
    return model_key.replace("/", "-")


TEI_RUNTIME_MODES: Dict[str, Dict[str, Any]] = {
    "cpu": {
        "label": "CPU",
        "image": "ghcr.io/huggingface/text-embeddings-inference:cpu-1.8",
        "requires_gpu": False,
        "description": "Default mode for CPU-only machines.",
    },
    "turing": {
        "label": "Turing (T4 / RTX 2000 series)",
        "image": "ghcr.io/huggingface/text-embeddings-inference:turing-1.8",
        "requires_gpu": True,
        "description": "Optimized build for NVIDIA Turing GPUs such as T4 or RTX 2000.",
    },
    "ampere_80": {
        "label": "Ampere 80 (A100 / A30)",
        "image": "ghcr.io/huggingface/text-embeddings-inference:1.8",
        "requires_gpu": True,
        "description": "Use on A100 or A30 class Ampere GPUs.",
    },
    "ampere_86": {
        "label": "Ampere 86 (A10 / A40)",
        "image": "ghcr.io/huggingface/text-embeddings-inference:86-1.8",
        "requires_gpu": True,
        "description": "Tune for Ampere 86 GPUs including A10, A40, and RTX A series.",
    },
    "ada_lovelace": {
        "label": "Ada Lovelace (RTX 4000 series)",
        "image": "ghcr.io/huggingface/text-embeddings-inference:89-1.8",
        "requires_gpu": True,
        "description": "Optimized for the RTX 4000 Ada Lovelace family.",
    },
    "hopper": {
        "label": "Hopper (H100, experimental)",
        "image": "ghcr.io/huggingface/text-embeddings-inference:hopper-1.8",
        "requires_gpu": True,
        "description": "Experimental build for NVIDIA Hopper GPUs (H100).",
    },
}
DEFAULT_TEI_RUNTIME_MODE = "cpu"

DOCKER_INSTALL_GUIDE_MD = """
**Docker is required to start Text Embeddings Inference.**

- **Windows**
  - [Docker Desktop installer](https://desktop.docker.com/win/main/amd64/Docker%20Desktop%20Installer.exe)
  - [Docker Desktop (Microsoft Store)](https://apps.microsoft.com/detail/xp8cbj40xlbwkx)
- **macOS**
  - [Apple silicon build](https://desktop.docker.com/mac/main/arm64/Docker.dmg)
  - [Intel build](https://desktop.docker.com/mac/main/amd64/Docker.dmg)
- **Linux**
  - [Docker Engine](https://docs.docker.com/engine/install/)
  - [Docker Desktop](https://docs.docker.com/desktop/setup/install/linux/)
"""

LINUX_GPU_TOOLKIT_MD = """
GPU modes on Linux also need the [NVIDIA Container Toolkit](https://docs.nvidia.com/datacenter/cloud-native/container-toolkit/latest/install-guide.html).
Verify the setup with:

```
sudo docker run --rm --runtime=nvidia --gpus all ubuntu nvidia-smi
```
"""


def load_tei_models_config() -> Dict[str, Dict[str, Any]]:
    try:
        with MODELS_CONFIG_PATH.open("r", encoding="utf-8") as fh:
            data = json.load(fh)
            if isinstance(data, dict):
                return data
    except FileNotFoundError:
        return {}
    except Exception:
        return {}
    return {}

def inject_global_styles() -> None:
    st.markdown(
        """
        <style>
        @import url('https://fonts.googleapis.com/css2?family=Inter:wght@400;500;600;700&display=swap');

        html, body, [class*="css"] {
            font-family: 'Inter', sans-serif !important;
            background-color: #0c0d12;
        }

        .stApp {
            background: #0c0d12;
        }

        [data-testid="stAppViewContainer"] > .main {
            padding: 0 3rem 4rem 3rem;
            background: radial-gradient(circle at center, rgba(32,34,48,0.55) 0%, rgba(12,13,18,0.95) 40%, #0c0d12 85%);
            color: #f5f6fb;
        }

        section[data-testid="stSidebar"] {
            background: #05060a;
            border-right: 1px solid rgba(255,255,255,0.05);
            padding: 1.5rem 1rem 1.25rem 1.25rem;
            width: 260px !important;
        }

        section[data-testid="stSidebar"] button[kind="secondary"] {
            border-radius: 12px;
            background: linear-gradient(135deg, rgba(79,81,255,0.35) 0%, rgba(59,61,122,0.35) 100%);
            color: #f5f6fb;
            border: 1px solid rgba(124,129,255,0.4);
            font-weight: 600;
            height: 48px;
        }

        section[data-testid="stSidebar"] button[kind="secondary"]:hover {
            background: linear-gradient(135deg, rgba(109,111,255,0.65) 0%, rgba(79,81,255,0.55) 100%);
        }

        .duck-sidebar-header {
            display: flex;
            align-items: center;
            gap: 0.6rem;
            margin-bottom: 1.5rem;
        }

        .duck-logo {
            width: 38px;
            height: 38px;
            border-radius: 12px;
            background: linear-gradient(140deg, #ff5f5f 0%, #ff7c3f 100%);
            display: flex;
            align-items: center;
            justify-content: center;
            font-size: 20px;
            box-shadow: 0 10px 22px rgba(255,120,120,0.25);
        }

        .duck-brand {
            display: flex;
            flex-direction: column;
            gap: 0.2rem;
        }

        .duck-brand .name {
            font-weight: 700;
            font-size: 0.95rem;
        }

        .duck-brand .badge {
            display: inline-flex;
            align-items: center;
            gap: 0.3rem;
            font-size: 0.70rem;
            padding: 0.2rem 0.45rem;
            border-radius: 999px;
            background: rgba(111,187,255,0.25);
            color: #8ed7ff;
            font-weight: 600;
            letter-spacing: 0.03em;
        }

        .sidebar-feedback-btn {
            width: 100%;
            border-radius: 30px;
            background: rgba(255,255,255,0.07);
            border: 1px solid rgba(255,255,255,0.08);
            padding: 0.65rem 1rem;
            text-align: center;
            color: #f5f6fb;
            font-weight: 600;
            cursor: pointer;
        }

        .sidebar-feedback-btn:hover {
            background: rgba(255,255,255,0.12);
        }

        .sidebar-footer {
            margin-top: auto;
            padding-top: 1.5rem;
            display: flex;
            flex-direction: column;
            gap: 0.9rem;
        }

        .sidebar-footer-icons {
            display: flex;
            align-items: center;
            justify-content: space-between;
            padding: 0 0.4rem;
            color: rgba(255,255,255,0.45);
        }

        .sidebar-footer-icons span {
            background: rgba(255,255,255,0.05);
            border-radius: 999px;
            padding: 0.35rem 0.75rem;
            font-size: 0.75rem;
            cursor: pointer;
            border: 1px solid rgba(255,255,255,0.08);
        }

        .sidebar-footer-icons span:hover {
            border-color: rgba(255,255,255,0.25);
            color: #f5f6fb;
        }

        .topbar-row {
            display: flex;
            align-items: center;
            justify-content: space-between;
            margin: 1.5rem 0 2rem;
        }

        .topbar-menu {
            font-size: 1.3rem;
            color: rgba(255,255,255,0.5);
            padding: 0.35rem 0.65rem;
            border-radius: 12px;
            border: 1px solid rgba(255,255,255,0.08);
            cursor: pointer;
        }

        .topbar-menu:hover {
            color: #f5f6fb;
            border-color: rgba(255,255,255,0.18);
        }

        .hero-wrapper {
            display: flex;
            gap: 1rem;
            margin-bottom: 1.5rem;
        }

        .hero-card {
            flex: 1;
            border-radius: 22px;
            background: linear-gradient(160deg, rgba(85,93,255,0.28) 0%, rgba(43,51,115,0.45) 100%);
            padding: 2rem 1.8rem;
            color: #f5f6fb;
            box-shadow: 0 20px 40px rgba(21,27,64,0.35);
        }

        .hero-card.secondary {
            background: linear-gradient(160deg, rgba(83,104,255,0.08) 0%, rgba(33,37,63,0.6) 100%);
            box-shadow: 0 14px 30px rgba(15,18,40,0.35);
        }

        .hero-card h3 {
            margin: 0 0 0.8rem;
            font-size: 1.25rem;
            font-weight: 700;
        }

        .hero-card p {
            margin: 0 0 1.6rem;
            color: rgba(229,232,255,0.75);
            font-size: 0.95rem;
            line-height: 1.6;
        }

        .hero-card a {
            display: inline-flex;
            align-items: center;
            justify-content: center;
            background: #4f56ff;
            color: #f5f6fb;
            padding: 0.55rem 1.5rem;
            border-radius: 14px;
            font-weight: 600;
            text-decoration: none;
            border: 1px solid rgba(255,255,255,0.18);
        }

        .hero-card a:hover {
            background: #636bff;
        }

        .tips-toggle {
            text-align: center;
            margin: 0.5rem 0 2rem;
            color: rgba(229,232,255,0.55);
            font-size: 0.85rem;
        }

        .suggestion-grid {
            display: grid;
            grid-template-columns: repeat(2, minmax(0, 1fr));
            gap: 0.8rem;
            margin-bottom: 2rem;
        }

        .suggestion-grid button {
            background: rgba(40,42,55,0.65);
            border-radius: 16px;
            padding: 0.75rem 1rem;
            color: rgba(235,238,255,0.9);
            font-weight: 600;
            border: 1px solid rgba(255,255,255,0.08);
            text-align: left;
        }

        .suggestion-grid button:hover {
            border-color: rgba(113,120,255,0.45);
            color: #f5f6fb;
        }

        div[data-testid="stChatMessage"] {
            background: rgba(27,30,43,0.65);
            border-radius: 18px;
            padding: 1.1rem 1.25rem;
            border: 1px solid rgba(255,255,255,0.06);
        }

        div[data-testid="stChatMessageUser"] {
            background: rgba(87,95,255,0.3);
            border: 1px solid rgba(140,147,255,0.45);
        }

        .stChatInputContainer {
            background: rgba(20,22,32,0.85);
            border-radius: 24px;
            border: 1px solid rgba(255,255,255,0.08);
            margin-top: 1.5rem;
        }

        .stChatInputContainer textarea {
            background: transparent !important;
            color: #f5f6fb !important;
            font-size: 0.98rem;
        }

        .stChatInputContainer button[kind="secondary"] {
            background: #4f56ff !important;
            color: #f5f6fb !important;
            border-radius: 16px !important;
        }

        div[data-testid="stExpander"] > div:first-child {
            background: transparent;
        }

        div[data-testid="stExpander"] > div:nth-child(2) {
            background: transparent;
        }
        </style>
        """,
        unsafe_allow_html=True,
    )


def render_left_navigation() -> None:
    with st.sidebar:
        st.markdown(
            """
            <div class="duck-sidebar-header">
                <div class="duck-logo">ðŸ¦†</div>
                <div class="duck-brand">
                    <span class="name">Duck.ai</span>
                    <span class="badge">FREE</span>
                </div>
            </div>
            """,
            unsafe_allow_html=True,
        )
        if st.button("+ New Chat", key="duck_new_chat", use_container_width=True):
            st.session_state.history = []
            st.session_state["_pending_question"] = None
            st.rerun()

        st.markdown(
            """
            <div class="sidebar-footer">
                <div class="sidebar-feedback-btn">Chia sáº» pháº£n há»“i</div>
                <div class="sidebar-footer-icons">
                    <span>âš™ï¸</span>
                    <span>âœ…</span>
                </div>
            </div>
            """,
            unsafe_allow_html=True,
        )


def render_topbar() -> None:
    st.session_state["_topbar_rendered"] = True
    left, _, right = st.columns([0.25, 0.5, 0.25])
    with left:
        chat_models = ["4o-mini", "gpt-4.1-mini", "gpt-4o", "gpt-4.1"]
        if st.session_state.chat_model not in chat_models:
            chat_models.insert(0, st.session_state.chat_model)
        selected = st.selectbox(
            "Model selector",
            options=chat_models,
            key="chat_model",
            label_visibility="collapsed",
        )
        st.session_state.chat_model = selected
    with right:
        st.markdown('<div class="topbar-menu">â˜°</div>', unsafe_allow_html=True)


def render_home_tiles() -> None:
    st.markdown(
        """
        <div class="hero-wrapper">
            <div class="hero-card">
                <h3>GPT-OSS 120B</h3>
                <p>Try the new model from OpenAI with faster reasoning and better multilingual support.</p>
                <a href="https://openai.com" target="_blank">Try Now</a>
            </div>
            <div class="hero-card secondary">
                <h3>Private conversations</h3>
                <p>Your chats stay on this machine and are never used to train third-party models.</p>
                <a href="https://openai.com/privacy" target="_blank">Learn More</a>
            </div>
        </div>
        <div class="tips-toggle">Hide Tips</div>
        """,
        unsafe_allow_html=True,
    )


SUGGESTION_PRESETS = [
    "Recommend a book about data science for beginners",
    "Improve your arguments for the research proposal",
    "Prepare for a purchase decision on lab equipment",
    "Identify product brands mentioned in the syllabus",
]


def render_suggestion_grid() -> None:
    st.markdown('<div class="suggestion-grid">', unsafe_allow_html=True)
    for idx, suggestion in enumerate(SUGGESTION_PRESETS):
        if st.button(suggestion, key=f"suggestion_{idx}"):
            st.session_state["_pending_question"] = suggestion
            st.experimental_rerun()
    st.markdown("</div>", unsafe_allow_html=True)


def render_advanced_panel(current_index_dir: Path) -> None:
    with st.expander("Advanced controls", expanded=False):
        render_sidebar()
        st.caption(
            f"DOCX trong `backend/data/raw`: {len(list_docx_files())} | "
            f"Index `{current_index_dir.relative_to(PROJECT_ROOT.parent)}` present: "
            f"{'yes' if index_exists(current_index_dir) else 'no'}"
        )


def get_tei_model_port(model_key: str) -> int:
    config = load_tei_models_config()
    config_key = resolve_tei_config_key(model_key)
    model_cfg = config.get(config_key, {})
    try:
        return int(model_cfg.get("port", 8800))
    except (TypeError, ValueError):
        return 8800


def sanitize_tei_container_name(model_key: str, runtime_key: str) -> str:
    base = f"{TEI_CONTAINER_PREFIX}{model_key}-{runtime_key}"
    slug = []
    for char in base.lower():
        if char.isalnum() or char == "-":
            slug.append(char)
        else:
            slug.append("-")
    sanitized = "".join(slug).strip("-")
    while "--" in sanitized:
        sanitized = sanitized.replace("--", "-")
    return sanitized[:63] or f"{TEI_CONTAINER_PREFIX}runtime"


def get_running_tei_containers() -> Tuple[List[str], Optional[str]]:
    try:
        result = subprocess.run(
            [
                "docker",
                "ps",
                "--format",
                "{{.Names}}",
                "--filter",
                f"name={TEI_CONTAINER_PREFIX}",
            ],
            capture_output=True,
            text=True,
            encoding="utf-8",
            errors="replace",
            check=False,
        )
    except FileNotFoundError:
        return [], "Docker CLI not found."
    except Exception as exc:
        return [], str(exc)

    if result.returncode != 0:
        detail = result.stderr.strip() or result.stdout.strip() or "Unknown Docker error."
        return [], detail

    names = [line.strip() for line in result.stdout.splitlines() if line.strip()]
    return names, None


def run_launch_tei(args: List[str]) -> subprocess.CompletedProcess[str]:
    command = [sys.executable, str(TOOLS_DIR / "launch_tei.py"), *args]
    return subprocess.run(
        command,
        cwd=str(BACKEND_ROOT),
        capture_output=True,
        text=True,
        encoding="utf-8",
        errors="replace",
    )


def run_backend_tool(
    script_name: str,
    *args: str,
    env: Optional[Dict[str, str]] = None,
) -> subprocess.CompletedProcess[str]:
    command = [sys.executable, str(TOOLS_DIR / script_name), *args]
    merged_env = os.environ.copy()
    if env:
        merged_env.update(env)
    return subprocess.run(
        command,
        cwd=str(BACKEND_ROOT),
        capture_output=True,
        text=True,
        encoding="utf-8",
        errors="replace",
        env=merged_env,
    )


def summarize_process(result: subprocess.CompletedProcess[str]) -> str:
    stdout = (result.stdout or "").strip()
    stderr = (result.stderr or "").strip()
    if stdout and stderr:
        return f"{stdout}\n{stderr}"
    return stdout or stderr or "No output."


def set_tei_base_url(port: int) -> str:
    base_url = f"http://localhost:{port}"
    os.environ["TEI_BASE_URL"] = base_url
    st.session_state.tei_base_url = base_url
    return base_url


def start_tei_runtime(model_key: str, runtime_key: str, port: int) -> Tuple[bool, str]:
    config_key = resolve_tei_config_key(model_key)
    args = [
        "--model",
        config_key,
        "--runtime",
        runtime_key,
        "--port",
        str(port),
        "--detach",
    ]
    result = run_launch_tei(args)
    success = result.returncode == 0
    runtime_label = TEI_RUNTIME_MODES.get(runtime_key, {}).get("label", runtime_key)
    if success:
        set_tei_base_url(port)
        message = f"Started TEI ({runtime_label}) on http://localhost:{port}."
    else:
        message = summarize_process(result)
    return success, message


def stop_tei_runtime(model_key: str, runtime_key: str) -> Tuple[bool, str]:
    config_key = resolve_tei_config_key(model_key)
    args = [
        "--model",
        config_key,
        "--runtime",
        runtime_key,
        "--stop",
    ]
    result = run_launch_tei(args)
    success = result.returncode == 0
    container_name = sanitize_tei_container_name(model_key, runtime_key)
    runtime_label = TEI_RUNTIME_MODES.get(runtime_key, {}).get("label", runtime_key)
    if success:
        message = (result.stdout or "").strip() or f"Stopped TEI container `{container_name}` ({runtime_label})."
    else:
        message = summarize_process(result)
    return success, message


def stop_all_tei_runtimes() -> Tuple[bool, str]:
    result = run_launch_tei(["--stop-all"])
    success = result.returncode == 0
    if success:
        message = (result.stdout or "").strip() or "No TEI containers were running."
    else:
        message = summarize_process(result)
    return success, message


def rebuild_index_from_docx_all(
    embedding_model: str,
    embedding_backend: str,
    base_url: str,
) -> Tuple[bool, str]:
    if embedding_backend != "tei":
        return False, "Global rebuild only supports the TEI embedding backend."

    convert_result = run_backend_tool("convert_docx_to_json.py")
    convert_message = summarize_process(convert_result)
    if convert_result.returncode != 0:
        return False, convert_message or "Failed while converting DOCX files."

    out_dir = resolve_index_dir(embedding_model)
    data_dir = BACKEND_ROOT / "data" / "processed-json"
    backend_choice = os.environ.get("VECTOR_INDEX_BACKEND") or os.environ.get("INDEX_BACKEND")

    build_args: List[str] = [
        "--model",
        embedding_model,
        "--base-url",
        base_url.rstrip("/"),
        "--data-dir",
        str(data_dir),
        "--out-dir",
        str(out_dir),
        "--langs",
        "vi",
        "en",
    ]
    if backend_choice:
        build_args.extend(["--backend", backend_choice])

    build_result = run_backend_tool("build_index.py", *build_args)
    build_message = summarize_process(build_result)
    success = build_result.returncode == 0

    messages = [msg for msg in (convert_message, build_message) if msg]
    combined_message = "\n\n".join(messages)

    if success:
        return True, combined_message or f"Finished rebuilding index at {out_dir}."
    return False, combined_message or "Failed while building the index."


def tei_backend_is_active(model_key: str, runtime_key: str) -> bool:
    containers, _ = get_running_tei_containers()
    return sanitize_tei_container_name(model_key, runtime_key) in containers


# -----------------------------------------------------------------------------#
# Utility helpers
# -----------------------------------------------------------------------------#


def safe_model_dir(model_name: str) -> str:
    slug = model_name.strip().lower().replace("/", "-")
    cleaned = []
    for ch in slug:
        if ch.isalnum() or ch in {"-", "_"}:
            cleaned.append(ch)
        else:
            cleaned.append("-")
    safe = "".join(cleaned).strip("-")
    while "--" in safe:
        safe = safe.replace("--", "-")
    return safe or "default"


def resolve_index_dir(model_name: str) -> Path:
    return INDEX_ROOT / safe_model_dir(model_name)


def get_embed_meta_path(index_dir: Path) -> Path:
    return index_dir / "embeddings.json"


def ensure_dirs() -> None:
    DATA_DIR.mkdir(parents=True, exist_ok=True)
    INDEX_ROOT.mkdir(parents=True, exist_ok=True)
    UPLOADS_ROOT.mkdir(parents=True, exist_ok=True)
    for ext in UPLOAD_ALLOWED_EXTS:
        (UPLOADS_ROOT / ext).mkdir(parents=True, exist_ok=True)
    for cfg in TEI_MODELS.values():
        local_dir: Path = cfg["local_dir"]
        local_dir.mkdir(parents=True, exist_ok=True)


def list_docx_files() -> List[Path]:
    return sorted(DATA_DIR.rglob("*.docx"))


def index_exists(index_dir: Path) -> bool:
    return backend_index_exists(index_dir)


def save_embed_meta(index_dir: Path, backend: str, model_name: str, chunk_mode: str) -> None:
    try:
        meta_path = get_embed_meta_path(index_dir)
        meta_path.parent.mkdir(parents=True, exist_ok=True)
        meta_path.write_text(
            json.dumps(
                {
                    "embedding_backend": backend,
                    "embedding_model": model_name,
                    "chunk_mode": chunk_mode,
                },
                ensure_ascii=False,
                indent=2,
            ),
            encoding="utf-8",
        )
    except Exception:
        pass  # Silently ignore metadata persistence issues


def load_embed_meta(index_dir: Path) -> Optional[Dict[str, Optional[str]]]:
    try:
        manifest_path = index_dir / "index_manifest.json"
        if manifest_path.exists():
            manifest = json.loads(manifest_path.read_text(encoding="utf-8"))
            model = manifest.get("model")
            return {
                "embedding_backend": "tei",
                "embedding_model": model,
                "chunk_mode": manifest.get("chunk_mode"),
                "vector_backend": manifest.get("backend"),
            }
        meta_path = get_embed_meta_path(index_dir)
        if meta_path.exists():
            meta = json.loads(meta_path.read_text(encoding="utf-8"))
            if isinstance(meta, dict):
                backend = meta.get("embedding_backend", "openai")
                model = meta.get("embedding_model")
                if model:
                    chunk = meta.get("chunk_mode")
                    return {
                        "embedding_backend": backend,
                        "embedding_model": model,
                        "chunk_mode": chunk,
                    }
            elif isinstance(meta, str):
                return {
                    "embedding_backend": "openai",
                    "embedding_model": meta,
                    "chunk_mode": None,
                }
    except Exception:
        pass
    return None


class TEIEmbeddings:
    """Client for text-embeddings-inference endpoint."""

    def __init__(self, base_url: str, model: str, api_key: Optional[str] = None) -> None:
        self.base_url = base_url.rstrip("/")
        self.model = model
        self.api_key = api_key
        self.session = requests.Session()

    def _request(self, texts: List[str]) -> List[List[float]]:
        if not texts:
            return []

        payload = {"input": texts, "model": self.model}
        headers = {"Content-Type": "application/json"}
        if self.api_key:
            headers["Authorization"] = f"Bearer {self.api_key}"

        response = self.session.post(
            f"{self.base_url}/embeddings",
            json=payload,
            headers=headers,
            timeout=120,
        )
        response.raise_for_status()
        data = response.json().get("data", [])
        return [item["embedding"] for item in data]

    def embed_documents(self, texts: List[str]) -> List[List[float]]:
        return self._request(texts)

    def embed_query(self, text: str) -> List[float]:
        result = self._request([text])
        return result[0] if result else []


def check_docker_cli() -> Tuple[bool, str]:
    try:
        result = subprocess.run(
            ["docker", "--version"],
            capture_output=True,
            text=True,
            encoding="utf-8",
            errors="replace",
            check=False,
        )
    except FileNotFoundError:
        return False, "Docker command not found in PATH."
    except Exception as exc:  # pragma: no cover - defensive
        return False, f"Failed to execute docker: {exc}"

    if result.returncode != 0:
        detail = result.stderr.strip() or result.stdout.strip() or "unknown error."
        return False, detail

    return True, result.stdout.strip()


def docker_supports_nvidia() -> Tuple[bool, Optional[str]]:
    try:
        result = subprocess.run(
            ["docker", "info", "--format", "{{json .Runtimes}}"],
            capture_output=True,
            text=True,
            encoding="utf-8",
            errors="replace",
            check=False,
        )
    except FileNotFoundError:
        return False, "Docker command not found in PATH."
    except Exception as exc:  # pragma: no cover - defensive
        return False, f"Failed to query docker: {exc}"

    if result.returncode != 0:
        detail = result.stderr.strip() or result.stdout.strip() or "unknown error."
        return False, detail

    if "nvidia" in result.stdout.lower():
        return True, None

    return False, "NVIDIA runtime not detected."


def make_embeddings_client(backend: str, model_name: str):
    if backend == "openai":
        return OpenAIEmbeddings(model=model_name)
    if backend == "tei":
        base_url = st.session_state.get("tei_base_url") or os.getenv("TEI_BASE_URL", "http://localhost:8080")
        api_key = os.getenv("TEI_API_KEY")
        return TEIEmbeddings(base_url=base_url, model=model_name, api_key=api_key)
    raise ValueError(f"Unsupported embedding backend: {backend}")


def tei_model_is_downloaded(model_key: str) -> bool:
    config = TEI_MODELS.get(model_key)
    if not config:
        return False
    required_path = config["local_dir"] / config["required_file"]
    return required_path.exists()


def run_tei_download(model_key: str) -> subprocess.CompletedProcess[str]:
    config = TEI_MODELS.get(model_key)
    if not config:
        raise ValueError(f"Unknown TEI model: {model_key}")

    script_path = config["download_script"]
    if not script_path.exists():
        raise FileNotFoundError(f"Missing download script: {script_path}")

    return subprocess.run(
        [sys.executable, str(script_path)],
        cwd=BACKEND_ROOT,
        check=False,
        capture_output=True,
        text=True,
        encoding="utf-8",
        errors="replace",
    )


# -----------------------------------------------------------------------------#
# Index utilities (backend pipeline)
# -----------------------------------------------------------------------------#


def detect_docx_languages() -> List[str]:
    languages: List[str] = []
    if not DATA_DIR.exists():
        return languages
    for child in sorted(DATA_DIR.iterdir()):
        if child.name.lower() == "uploads":
            continue
        if not child.is_dir():
            continue
        if any(child.glob("*.docx")):
            languages.append(child.name)
    return languages


def backend_index_exists(index_dir: Path) -> bool:
    manifest = index_dir / "index_manifest.json"
    if not manifest.exists():
        return False
    faiss_path = index_dir / "index.faiss"
    brute_path = index_dir / "vectors.npy"
    return faiss_path.exists() or brute_path.exists()


def run_backend_pipeline(model_name: str, langs: List[str], base_url: Optional[str], out_dir: Path) -> Tuple[bool, str]:
    script = TOOLS_DIR / "ingest_docx_pipeline.py"
    if not script.exists():
        return False, f"Pipeline script not found: {script}"

    args = [
        sys.executable,
        str(script),
        "--model",
        model_name,
        "--out-dir",
        str(out_dir),
    ]
    if base_url:
        args.extend(["--base-url", base_url])
    if langs:
        args.extend(["--langs", *langs])

    args.extend(["--batch-size", "8"])

    existing_proc = st.session_state.get("backend_pipeline_proc")
    if existing_proc and existing_proc.poll() is None:
        try:
            existing_proc.terminate()
            existing_proc.wait(timeout=5)
        except subprocess.TimeoutExpired:
            existing_proc.kill()
        except Exception:
            pass
    st.session_state["backend_pipeline_proc"] = None
    SOFT_PID_REGISTRY.pop("backend_pipeline", None)

    try:
        result = subprocess.Popen(
            args,
            cwd=str(PROJECT_ROOT.parent),
            stdout=subprocess.PIPE,
            stderr=subprocess.PIPE,
            text=True,
            encoding="utf-8",
            errors="replace",
        )
    except Exception as exc:
        return False, f"Failed to launch pipeline: {exc}"

    st.session_state["backend_pipeline_proc"] = result
    SOFT_PID_REGISTRY["backend_pipeline"] = result.pid
    try:
        stdout, stderr = result.communicate()
    finally:
        st.session_state["backend_pipeline_proc"] = None
        SOFT_PID_REGISTRY.pop("backend_pipeline", None)

    output = (stdout or "").strip()
    error = (stderr or "").strip()
    message = output if output else error if error else "No output."
    success = result.returncode == 0
    return success, message


def _l2_normalize(array: np.ndarray) -> np.ndarray:
    norms = np.linalg.norm(array, axis=1, keepdims=True)
    norms[norms == 0] = 1.0
    return array / norms


def _load_backend_resources(index_dir: Path) -> Dict[str, Any]:
    manifest_path = index_dir / "index_manifest.json"
    meta_path = index_dir / "meta.jsonl"
    if not manifest_path.exists() or not meta_path.exists():
        raise FileNotFoundError("Backend index manifest or meta.jsonl is missing.")

    manifest = json.loads(manifest_path.read_text(encoding="utf-8"))
    metas: List[Dict[str, Any]] = []
    with meta_path.open("r", encoding="utf-8") as stream:
        for line in stream:
            if line.strip():
                metas.append(json.loads(line))

    backend_type = manifest.get("backend", "faiss")
    faiss_index = None
    vectors = None
    if backend_type == "faiss":
        if faiss is None:
            raise RuntimeError("faiss is required to load this index but is not available.")
        index_path = index_dir / "index.faiss"
        if not index_path.exists():
            raise FileNotFoundError(f"FAISS index missing: {index_path}")
        faiss_index = faiss.read_index(str(index_path))
    elif backend_type == "bruteforce":
        vector_path = index_dir / "vectors.npy"
        if not vector_path.exists():
            raise FileNotFoundError(f"Vector file missing: {vector_path}")
        vectors = np.load(vector_path)
    else:
        raise RuntimeError(f"Unsupported backend type: {backend_type}")

    return {
        "manifest": manifest,
        "metas": metas,
        "faiss_index": faiss_index,
        "vectors": vectors,
    }


def ensure_backend_index_cache(model_name: str) -> Dict[str, Any]:
    key = safe_model_dir(model_name)
    cache: Dict[str, Dict[str, Any]] = st.session_state.setdefault("backend_index_cache", {})
    if key not in cache:
        cache[key] = _load_backend_resources(resolve_index_dir(model_name))
    return cache[key]


def invalidate_backend_index_cache(model_name: Optional[str] = None) -> None:
    if "backend_index_cache" not in st.session_state:
        return
    if model_name is None:
        st.session_state.backend_index_cache = {}
        return
    key = safe_model_dir(model_name)
    st.session_state.backend_index_cache.pop(key, None)


def embed_query_vector(question: str, embedding_backend: str, embedding_model: str) -> np.ndarray:
    embeddings = make_embeddings_client(embedding_backend, embedding_model)
    vector = embeddings.embed_query(question)
    array = np.asarray([vector], dtype="float32")
    return _l2_normalize(array)


def search_backend_index(question: str, top_k: int) -> List[Dict[str, Any]]:
    resources = ensure_backend_index_cache(st.session_state.embed_model)
    qvec = embed_query_vector(question, st.session_state.embedding_backend, st.session_state.embed_model)

    backend_type = resources["manifest"].get("backend", "faiss")
    metas = resources["metas"]
    total = len(metas)
    if total == 0:
        return []
    k = min(top_k, total)
    results: List[Tuple[float, int]] = []

    if backend_type == "faiss":
        faiss_index = resources["faiss_index"]
        if faiss_index is None:
            raise RuntimeError("FAISS index not loaded.")
        distances, indices = faiss_index.search(qvec, k)
        for score, idx in zip(distances[0], indices[0]):
            if idx == -1:
                continue
            results.append((float(score), int(idx)))
    elif backend_type == "bruteforce":
        vectors = resources["vectors"]
        if vectors is None:
            raise RuntimeError("Vector array not loaded.")
        sims = (vectors @ qvec.T).reshape(-1)
        top_indices = np.argsort(-sims)[:k]
        for idx in top_indices:
            results.append((float(sims[int(idx)]), int(idx)))
    else:
        raise RuntimeError(f"Unsupported backend: {backend_type}")

    ranked: List[Dict[str, Any]] = []
    for score, idx in results:
        if idx < 0 or idx >= total:
            continue
        meta = metas[idx]
        ranked.append({"score": score, "meta": meta})
    return ranked


def format_backend_context(chunks: List[Dict[str, Any]]) -> str:
    parts: List[str] = []
    for item in chunks:
        meta = item["meta"]
        source = meta.get("source_filename") or meta.get("filename") or meta.get("doc_id", "unknown")
        heading = meta.get("section_heading") or meta.get("primary_heading") or meta.get("breadcrumbs") or ""
        prefix = f"[{source}] "
        if heading:
            prefix += f"{heading} "
        text = meta.get("text") or ""
        parts.append(f"{prefix}{text}")
    return "\n\n---\n\n".join(parts)


def call_llm(chat_model: str, question: str, context: str) -> str:
    prompt = ChatPromptTemplate.from_messages(
        [
            (
                "system",
                "You are a precise assistant. Only answer using the provided context. "
                "If the answer is not contained there, reply that you do not know.",
            ),
            ("human", "Question:\n{question}\n\nContext:\n{context}\n\nAnswer in Vietnamese."),
        ]
    )

    llm = ChatOpenAI(model=chat_model, temperature=0)
    messages = prompt.format_messages(question=question, context=context)
    response = llm.invoke(messages)
    return response.content if hasattr(response, "content") else str(response)


# -----------------------------------------------------------------------------#
# Streamlit helpers
# -----------------------------------------------------------------------------#


def apply_material_theme() -> None:
    """Inject a pastel blue Material-inspired theme into the Streamlit app."""
    st.markdown(
        """
        <style>
        :root {
            --primary-50: #e8f1ff;
            --primary-100: #d4e4ff;
            --primary-200: #afc9ff;
            --primary-300: #8eb9ff;
            --primary-400: #6ea2f6;
            --primary-500: #5a8dee;
            --primary-600: #4779d7;
            --primary-700: #355fb9;
            --text-primary: #1f2a40;
            --text-muted: #4e5d7b;
            --surface: #ffffff;
            --surface-muted: #f4f7ff;
            --border-soft: rgba(90, 141, 238, 0.2);
            --shadow-soft: 0 12px 28px rgba(90, 141, 238, 0.16);
        }

        .stApp {
            background: linear-gradient(155deg, var(--surface-muted) 0%, #edf4ff 40%, #ffffff 100%);
            color: var(--text-primary);
            font-family: "Google Sans", "Segoe UI", sans-serif;
        }

        .stApp [data-testid="stToolbar"] {
            background: transparent;
        }

        div[data-testid="stSidebar"] {
            background: linear-gradient(180deg, rgba(142, 185, 255, 0.35) 0%, rgba(236, 244, 255, 0.95) 100%);
            border-right: 1px solid var(--border-soft);
            box-shadow: 4px 0 16px rgba(82, 122, 196, 0.05);
        }

        div[data-testid="stSidebar"] > div:first-child {
            padding-top: 1.5rem;
        }

        div[data-testid="stSidebar"] label {
            color: var(--text-muted);
            font-weight: 600;
            text-transform: uppercase;
            font-size: 0.72rem;
            letter-spacing: 0.08em;
        }

        div[data-testid="stSidebar"] .stSelectbox,
        div[data-testid="stSidebar"] .stSlider,
        div[data-testid="stSidebar"] .stTextInput {
            padding: 0.75rem;
            border-radius: 18px;
            background: rgba(255, 255, 255, 0.85);
            box-shadow: 0 10px 25px rgba(90, 141, 238, 0.08);
            border: 1px solid var(--border-soft);
            margin-bottom: 1rem;
        }

        div[data-testid="stSidebar"] .stSelectbox:hover,
        div[data-testid="stSidebar"] .stSlider:hover,
        div[data-testid="stSidebar"] .stTextInput:hover {
            box-shadow: 0 18px 32px rgba(90, 141, 238, 0.14);
        }

        div[data-testid="stSidebar"] input,
        div[data-testid="stSidebar"] textarea {
            border-radius: 12px !important;
            border: 1px solid transparent !important;
            background-color: rgba(255, 255, 255, 0.95);
        }

        div[data-testid="stSidebar"] .stSlider > div[role="slider"] {
            color: var(--primary-600);
        }

        div.stButton > button {
            border-radius: 999px;
            background: linear-gradient(135deg, var(--primary-500), var(--primary-600));
            color: #ffffff;
            font-weight: 600;
            letter-spacing: 0.01em;
            border: none;
            box-shadow: 0 12px 24px rgba(90, 141, 238, 0.24);
        }

        div.stButton > button[aria-label="Start TEI"] {
            background: linear-gradient(135deg, #16a34a, #15803d);
            border: 1px solid #166534;
        }

        div.stButton > button[aria-label="Start TEI"]:hover {
            background: linear-gradient(135deg, #15803d, #166534);
            box-shadow: 0 14px 28px rgba(21, 128, 61, 0.28);
        }

        div.stButton > button[aria-label="Stop TEI"] {
            background: linear-gradient(135deg, #dc2626, #b91c1c);
            border: 1px solid #991b1b;
        }

        div.stButton > button[aria-label="Stop TEI"]:hover {
            background: linear-gradient(135deg, #b91c1c, #991b1b);
            box-shadow: 0 14px 28px rgba(220, 38, 38, 0.28);
        }

        div.stButton > button:hover:not([aria-label="Start TEI"]):not([aria-label="Stop TEI"]) {
            background: linear-gradient(135deg, var(--primary-600), var(--primary-700));
            box-shadow: 0 16px 28px rgba(71, 121, 215, 0.32);
        }

        section.main > div {
            padding-top: 1rem;
        }

        .hero-banner {
            background: linear-gradient(145deg, rgba(255, 255, 255, 0.9), rgba(232, 241, 255, 0.9));
            border: 1px solid var(--border-soft);
            border-radius: 24px;
            padding: 1.8rem 2rem;
            box-shadow: var(--shadow-soft);
            margin-bottom: 1.5rem;
        }

        .hero-banner h2 {
            margin-bottom: 0.4rem;
            color: var(--primary-600);
            font-weight: 700;
        }

        .hero-banner p {
            margin: 0;
            color: var(--text-muted);
            font-size: 0.95rem;
        }

        div[data-testid="stChatMessage"] {
            background: rgba(255, 255, 255, 0.92);
            border-radius: 20px;
            padding: 1.1rem 1.4rem;
            margin-bottom: 1rem;
            border: 1px solid rgba(90, 141, 238, 0.14);
            box-shadow: 0 12px 28px rgba(15, 76, 129, 0.08);
        }

        div[data-testid="stChatMessage"] pre {
            background: #f8fbff !important;
            border-radius: 14px;
        }

        div[data-testid="stChatMessage"] .stExpander {
            border-radius: 16px;
            background: rgba(232, 241, 255, 0.7);
        }

        div[data-testid="stChatMessage"] .stExpander:hover {
            background: rgba(142, 185, 255, 0.18);
        }

        div[data-testid="stSidebar"] div[data-testid="stRadio"] {
            padding: 0.6rem 0.75rem;
            background: rgba(255, 255, 255, 0.85);
            border-radius: 18px;
            border: 1px solid var(--border-soft);
            box-shadow: 0 10px 22px rgba(90, 141, 238, 0.08);
            margin-bottom: 1rem;
        }

        div[data-testid="stSidebar"] div[data-testid="stRadio"] label {
            font-size: 0.9rem;
            color: var(--text-primary);
            font-weight: 600;
            padding: 0.25rem 0.45rem;
            border-radius: 12px;
        }

        div[data-testid="stSidebar"] div[data-testid="stRadio"] label:hover {
            background: rgba(142, 185, 255, 0.16);
        }

        .runtime-chip {
            display: inline-block;
            padding: 0.35rem 0.9rem;
            border-radius: 999px;
            font-size: 0.85rem;
            font-weight: 600;
            letter-spacing: 0.01em;
            margin-bottom: 0.35rem;
        }

        .runtime-chip--ok {
            background: rgba(118, 204, 145, 0.22);
            color: #2f7a47;
            border: 1px solid rgba(118, 204, 145, 0.42);
        }

        .runtime-chip--pending {
            background: rgba(255, 200, 142, 0.22);
            color: #a35a00;
            border: 1px solid rgba(255, 200, 142, 0.4);
        }

        div[data-testid="stChatInput"] textarea {
            border-radius: 18px;
            border: 1px solid var(--border-soft);
            box-shadow: 0 12px 24px rgba(90, 141, 238, 0.12);
            background: rgba(255, 255, 255, 0.95);
        }
        </style>
        """,
        unsafe_allow_html=True,
    )


def init_session() -> None:
    existing_proc = st.session_state.get("backend_pipeline_proc")
    if existing_proc and existing_proc.poll() is None:
        try:
            existing_proc.terminate()
            existing_proc.wait(timeout=5)
        except subprocess.TimeoutExpired:
            existing_proc.kill()
        except Exception:
            pass
    st.session_state["backend_pipeline_proc"] = None
    SOFT_PID_REGISTRY.pop("backend_pipeline", None)

    if "history" not in st.session_state:
        st.session_state.history = []
    if "retriever_k" not in st.session_state:
        st.session_state.retriever_k = 4
    if "chat_model" not in st.session_state:
        st.session_state.chat_model = DEFAULT_CHAT_MODEL
    if "chunk_mode" not in st.session_state:
        st.session_state.chunk_mode = DEFAULT_CHUNK_MODE
    if "embedding_backend" not in st.session_state:
        if DEFAULT_EMBED_MODEL and DEFAULT_EMBED_MODEL in TEI_MODELS:
            st.session_state.embedding_backend = "tei"
        else:
            st.session_state.embedding_backend = "openai"
    if "embed_model" not in st.session_state:
        if DEFAULT_EMBED_MODEL:
            st.session_state.embed_model = DEFAULT_EMBED_MODEL
        else:
            st.session_state.embed_model = (
                list(TEI_MODELS.keys())[0] if st.session_state.embedding_backend == "tei" else OPENAI_EMBED_MODELS[0]
            )
    if "openai_key" not in st.session_state:
        st.session_state.openai_key = os.getenv("OPENAI_API_KEY") or ""
    if "download_feedback" not in st.session_state:
        st.session_state.download_feedback = None
    if "upload_feedback" not in st.session_state:
        st.session_state.upload_feedback = None
    if "tei_runtime_mode" not in st.session_state:
        st.session_state.tei_runtime_mode = DEFAULT_TEI_RUNTIME_MODE
    if "tei_control_feedback" not in st.session_state:
        st.session_state.tei_control_feedback = None
    if "tei_base_url" not in st.session_state:
        st.session_state.tei_base_url = os.getenv("TEI_BASE_URL")

    if st.session_state.embedding_backend == "openai" and st.session_state.embed_model not in OPENAI_EMBED_MODELS:
        st.session_state.embed_model = OPENAI_EMBED_MODELS[0]
    if st.session_state.embedding_backend == "tei" and st.session_state.embed_model not in TEI_MODELS:
        st.session_state.embed_model = list(TEI_MODELS.keys())[0]
    if "backend_index_cache" not in st.session_state:
        st.session_state.backend_index_cache = {}


def render_settings_body() -> None:
    st.title("Settings")

    st.text_input(
        "OpenAI API Key",
        type="password",
        help="Enter your API key here if you do not have a .env file.",
        key="openai_key",
    )
    if st.session_state.openai_key:
        os.environ["OPENAI_API_KEY"] = st.session_state.openai_key

    backend_options = list(EMBED_BACKENDS.keys())
    previous_backend = st.session_state.embedding_backend
    st.selectbox(
        "Embedding source",
        backend_options,
        format_func=lambda key: EMBED_BACKENDS[key],
        key="embedding_backend",
    )
    if st.session_state.embedding_backend != previous_backend:
        st.session_state.embed_model = (
            OPENAI_EMBED_MODELS[0] if st.session_state.embedding_backend == "openai" else list(TEI_MODELS.keys())[0]
        )
        st.rerun()

    if not st.session_state.get("_topbar_rendered"):
        if st.session_state.embedding_backend == "openai":
            options = OPENAI_EMBED_MODELS
            if st.session_state.embed_model not in options:
                st.session_state.embed_model = options[0]
            st.selectbox(
                "Embedding model",
                options=options,
                help="Rebuild the index after changing the embedding model.",
                key="embed_model",
            )
        else:
            options = list(TEI_MODELS.keys())
            if st.session_state.embed_model not in options:
                st.session_state.embed_model = options[0]
            st.selectbox(
                "Embedding model",
                options=options,
                format_func=lambda key: TEI_MODELS[key]["display"],
                key="embed_model",
            )
    else:
        if st.session_state.embedding_backend == "tei":
            options = list(TEI_MODELS.keys())
            if st.session_state.embed_model not in options:
                st.session_state.embed_model = options[0]
            st.selectbox(
                "Embedding model",
                options=options,
                format_func=lambda key: TEI_MODELS[key]["display"],
                key="embed_model",
            )
        else:
            st.selectbox(
                "Embedding model",
                options=OPENAI_EMBED_MODELS,
                help="Rebuild the index after changing the embedding model.",
                key="embed_model",
            )

        runtime_options = list(TEI_RUNTIME_MODES.keys())
        if st.session_state.tei_runtime_mode not in runtime_options:
            st.session_state.tei_runtime_mode = DEFAULT_TEI_RUNTIME_MODE
        st.selectbox(
            "TEI runtime mode",
            options=runtime_options,
            format_func=lambda key: TEI_RUNTIME_MODES[key]["label"],
            key="tei_runtime_mode",
        )
        runtime_mode = st.session_state.tei_runtime_mode
        runtime_info = TEI_RUNTIME_MODES[runtime_mode]

        docker_ok, docker_detail = check_docker_cli()
        running_containers: List[str] = []
        running_error: Optional[str] = None
        if docker_ok:
            st.success(f"Docker detected ({docker_detail}).")
            running_containers, running_error = get_running_tei_containers()
        else:
            st.error("Docker CLI not available. Install Docker to run TEI containers.")
            st.markdown(DOCKER_INSTALL_GUIDE_MD)

        st.caption(f"Docker image: `{runtime_info['image']}`")
        if runtime_info.get("description"):
            st.caption(runtime_info["description"])

        if runtime_info.get("requires_gpu"):
            st.info("This runtime requires an NVIDIA GPU.")
            if platform.system().lower() == "linux" and docker_ok:
                nvidia_ok, nvidia_detail = docker_supports_nvidia()
                if nvidia_ok:
                    st.success("NVIDIA Container Toolkit detected via `docker info`.")
                else:
                    st.error("NVIDIA Container Toolkit not detected. Install it before launching GPU runtimes on Linux.")
                    st.markdown(LINUX_GPU_TOOLKIT_MD)
                    if nvidia_detail and "not detected" not in nvidia_detail.lower():
                        st.caption(nvidia_detail)
        elif not docker_ok:
            st.info("Docker installation is required before running the TEI backend.")

        model_key = st.session_state.embed_model
        config = TEI_MODELS[model_key]
        downloaded = tei_model_is_downloaded(model_key)
        port_value = get_tei_model_port(model_key)
        container_name = sanitize_tei_container_name(model_key, runtime_mode)
        set_tei_base_url(port_value)
        is_running = docker_ok and container_name in running_containers

        st.caption(f"TEI endpoint URL: `http://localhost:{port_value}/embed`")
        if running_error:
            st.warning(f"Could not inspect Docker containers: {running_error}")
        elif is_running:
            st.success(f"Container `{container_name}` is running.")
        elif docker_ok:
            st.info("No TEI container is running right now.")

        button_label = "Stop TEI" if is_running else "Start TEI"
        button_disabled = not docker_ok
        if st.button(
            button_label,
            key=f"tei-toggle-{model_key}-{runtime_mode}",
            use_container_width=True,
            type="primary",
            disabled=button_disabled,
        ):
            if is_running:
                success, message = stop_tei_runtime(model_key, runtime_mode)
            else:
                success, message = start_tei_runtime(model_key, runtime_mode, port_value)
            st.session_state.tei_control_feedback = ("success" if success else "error", message)
            st.rerun()

        control_feedback = st.session_state.tei_control_feedback
        if control_feedback:
            status, message = control_feedback
            if status == "success":
                st.success(message)
            else:
                st.error(message)
            st.session_state.tei_control_feedback = None

        st.markdown("**Local Text-Embeddings-Inference model**")
        st.caption(config["display"])
        status_col, info_col = st.columns([0.25, 0.75])
        with status_col:
            status_text = "ready" if downloaded else "download"
            st.markdown(f"`{status_text}`")
        with info_col:
            st.caption(f"Path: `{config['local_dir']}`")
            if not downloaded:
                if st.button("Download model", key=f"download-{model_key}"):
                    result = run_tei_download(model_key)
                    success = result.returncode == 0 and tei_model_is_downloaded(model_key)
                    if success:
                        st.session_state.download_feedback = (
                            "success",
                            f"Downloaded {config['display']} successfully.",
                        )
                    else:
                        detail = (result.stderr or "").strip() or (result.stdout or "").strip() or "No log output."
                        st.session_state.download_feedback = (
                            "error",
                            f"Failed to download {config['display']} (code {result.returncode}). {detail}",
                        )
                    st.rerun()

    feedback = st.session_state.download_feedback
    if feedback:
        status, message = feedback
        if status == "success":
            st.success(message)
        else:
            st.error(message)
        st.session_state.download_feedback = None

    chat_models = ["gpt-4o-mini", "gpt-4o", "gpt-4.1-mini", "gpt-4.1"]
    if st.session_state.chat_model not in chat_models:
        st.session_state.chat_model = chat_models[0]
    st.selectbox(
        "Chat model",
        options=chat_models,
        key="chat_model",
    )

    st.selectbox(
        "Chunk mode",
        options=list(CHUNK_MODES.keys()),
        format_func=lambda key: CHUNK_MODES[key],
        key="chunk_mode",
    )

    st.slider(
        "Top-k passages",
        min_value=2,
        max_value=10,
        step=1,
        key="retriever_k",
    )

def render_sidebar_quick_actions() -> None:
    st.subheader("Data")

    uploaded_files = st.file_uploader(
        "Upload documents",
        type=sorted(UPLOAD_ALLOWED_EXTS),
        accept_multiple_files=True,
        help="Files are saved to `backend/data/raw/uploads/<ext>/`.",
    )
    if uploaded_files:
        saved_paths = []
        errors = []
        for file in uploaded_files:
            suffix = Path(file.name).suffix.lower()
            ext = suffix.lstrip(".")
            if ext not in UPLOAD_ALLOWED_EXTS:
                errors.append(f"Unsupported file type: {file.name}")
                continue
            target_dir = (UPLOADS_ROOT / ext)
            target_dir.mkdir(parents=True, exist_ok=True)
            stem = Path(file.name).stem.strip()
            safe_stem = "".join(ch if ch.isalnum() or ch in {"-", "_"} else "-" for ch in stem).strip("-") or "document"
            unique_name = f"{safe_stem}-{uuid4().hex[:8]}{suffix}"
            target_path = target_dir / unique_name
            try:
                with open(target_path, "wb") as out_file:
                    out_file.write(file.getbuffer())
                saved_paths.append(target_path)
            except Exception as exc:
                errors.append(f"Failed to save {file.name}: {exc}")
        messages = []
        if saved_paths:
            rel_paths = [str(path.relative_to(PROJECT_ROOT.parent)) for path in saved_paths]
            messages.append("Saved: " + ", ".join(rel_paths))
        if errors:
            messages.extend(errors)
        if saved_paths and errors:
            status = "warning"
        elif saved_paths:
            status = "success"
        elif errors:
            status = "error"
        else:
            status = "info"
        st.session_state.upload_feedback = (status, " | ".join(messages) if messages else "No files processed.")
        st.rerun()

    upload_feedback = st.session_state.upload_feedback
    if upload_feedback:
        status, message = upload_feedback
        if status == "success":
            st.success(message)
        elif status == "warning":
            st.warning(message)
        elif status == "info":
            st.info(message)
        else:
            st.error(message)
        st.session_state.upload_feedback = None

    st.divider()
    current_index_dir = resolve_index_dir(st.session_state.embed_model)
    docx_langs = detect_docx_languages()
    if st.button("Rebuild backend index", use_container_width=True):
        if st.session_state.embedding_backend != "tei":
            st.error("Chá»©c nÄƒng nÃ y chá»‰ kháº£ dá»¥ng khi chá»n Local Text-Embeddings-Inference.")
        elif not tei_model_is_downloaded(st.session_state.embed_model):
            st.error("Model TEI Ä‘Æ°á»£c chá»n chÆ°a Ä‘Æ°á»£c táº£i.")
        else:
            base_url = st.session_state.get("tei_base_url") or os.getenv("TEI_BASE_URL")
            if not base_url:
                st.error("KhÃ´ng tÃ¬m tháº¥y TEI base URL. HÃ£y cháº¡y hoáº·c cáº¥u hÃ¬nh TEI trÆ°á»›c.")
            else:
                with st.spinner("Äang cháº¡y pipeline DOCX â†’ JSON â†’ Index..."):
                    langs = docx_langs or ["vi"]
                    success, message = run_backend_pipeline(
                        st.session_state.embed_model,
                        langs,
                        base_url,
                        current_index_dir,
                    )
                    if success:
                        invalidate_backend_index_cache(st.session_state.embed_model)
                        st.success("Pipeline hoÃ n táº¥t.")
                    else:
                        st.error(f"Pipeline tháº¥t báº¡i: {message}")

    if st.button("Rebuild index from DOCX (all languages)", use_container_width=True):
        if st.session_state.embedding_backend != "tei":
            st.error("Switch embedding backend to TEI to rebuild from DOCX.")
        else:
            runtime_mode = st.session_state.tei_runtime_mode
            model_key = st.session_state.embed_model
            base_url = st.session_state.get("tei_base_url") or os.getenv("TEI_BASE_URL")
            if not base_url:
                st.error("TEI base URL is not configured. Start the TEI runtime first.")
            elif not tei_backend_is_active(model_key, runtime_mode):
                st.error("TEI runtime is not running. Start it before rebuilding.")
            else:
                with st.spinner("Converting DOCX files and rebuilding index..."):
                    success, message = rebuild_index_from_docx_all(
                        embedding_model=model_key,
                        embedding_backend=st.session_state.embedding_backend,
                        base_url=base_url,
                    )
                if success:
                    st.success(message or "Finished rebuilding index from DOCX files.")
                else:
                    st.error(message or "Failed to rebuild index from DOCX files.")

    if st.button("Clear chat history", use_container_width=True):
        st.session_state.history = []
        st.rerun()

    st.divider()
    index_state = "yes" if index_exists(current_index_dir) else "no"
    st.caption(
        f"DOCX trong `backend/data/raw`: {len(list_docx_files())} | "
        f"Index `{current_index_dir.relative_to(PROJECT_ROOT.parent)}` present: {index_state}"
    )

    emb_used = load_embed_meta(current_index_dir)
    if emb_used:
        backend_label = EMBED_BACKENDS.get(emb_used.get("embedding_backend", "openai"), "Unknown")
        chunk_value = emb_used.get("chunk_mode")
        chunk_label = CHUNK_MODES.get(chunk_value, chunk_value if chunk_value else "unknown")
        st.caption(f"Index built with: {backend_label} / {emb_used['embedding_model']} / {chunk_label}")
        if (
            emb_used.get("embedding_backend") != st.session_state.embedding_backend
            or emb_used.get("embedding_model") != st.session_state.embed_model
            or emb_used.get("chunk_mode") != st.session_state.chunk_mode
        ):
            st.warning("The current embedding selection differs from the index. Rebuild to avoid inconsistencies.")


def main() -> None:
    load_dotenv()
    ensure_dirs()
    init_session()

    st.set_page_config(page_title="RAG over PDFs", page_icon=":books:", layout="wide")
    apply_material_theme()

    with st.sidebar:
        render_sidebar()

    st.title("NEU Research Chatbot")
    st.markdown(
        """
        <div class="hero-banner">
            <h2>Trá»£ lÃ½ nghiÃªn cá»©u NEU</h2>
            <p>Khai thÃ¡c tri thá»©c trong tÃ i liá»‡u PDF cá»§a báº¡n vá»›i giao diá»‡n Material pastel xanh dÆ°Æ¡ng nháº¹ nhÃ ng.</p>
        </div>
        """,
        unsafe_allow_html=True,
    )

    current_index_dir = resolve_index_dir(st.session_state.embed_model)

    if st.session_state.embedding_backend == "openai" and not st.session_state.openai_key:
        st.info("No OpenAI API key detected. Enter it in the sidebar or the `.env` file before generating embeddings.")
    elif st.session_state.embedding_backend == "tei":
        runtime_mode = st.session_state.tei_runtime_mode
        model_key = st.session_state.embed_model
        if not tei_backend_is_active(model_key, runtime_mode):
            st.warning("The Docker-based TEI service is not running. Start it from the sidebar before continuing.")
    if not index_exists(current_index_dir):
        st.warning("ChÆ°a phÃ¡t hiá»‡n index backend. HÃ£y cháº¡y pipeline trong sidebar Ä‘á»ƒ dá»±ng láº¡i index tá»« DOCX.")

    for turn in st.session_state.history:
        with st.chat_message(turn["role"]):
            st.markdown(turn["content"])
            if turn.get("sources"):
                with st.expander("Sources"):
                    for idx, source in enumerate(turn["sources"], start=1):
                        st.markdown(f"**{idx}.** `{source['source']}` (p.{source.get('page', '?')})")
                        if source.get("snippet"):
                            st.caption(source["snippet"])

    user_question = st.chat_input("Ask something about your documents...")
    if user_question:
        st.session_state.history.append({"role": "user", "content": user_question})

        runtime_mode = st.session_state.tei_runtime_mode
        tei_model_key = st.session_state.embed_model

        if st.session_state.embedding_backend != "tei":
            st.session_state.history.append({
                "role": "assistant",
                "content": "Truy há»“i thá»‘ng nháº¥t chá»‰ há»— trá»£ khi chá»n Local Text-Embeddings-Inference. HÃ£y chuyá»ƒn sang Local Text-Embeddings-Inference trong sidebar.",
            })
            st.rerun()

        if st.session_state.embedding_backend == "tei" and not tei_backend_is_active(tei_model_key, runtime_mode):
            st.session_state.history.append({
                "role": "assistant",
                "content": "TEI chÆ°a cháº¡y. Khá»Ÿi Ä‘á»™ng container trong sidebar trÆ°á»›c khi tiáº¿p tá»¥c.",
            })
            st.rerun()

        if not tei_model_is_downloaded(tei_model_key):
            st.session_state.history.append({
                "role": "assistant",
                "content": "Model TEI Ä‘Æ°á»£c chá»n chÆ°a Ä‘Æ°á»£c táº£i. Thá»±c hiá»‡n táº£i model trong sidebar.",
            })
            st.rerun()

        if not backend_index_exists(current_index_dir):
            st.session_state.history.append({
                "role": "assistant",
                "content": "ChÆ°a cÃ³ index backend. Nháº¥n \"Rebuild backend index\" trong sidebar sau khi chuáº©n bá»‹ DOCX.",
            })
            st.rerun()

        try:
            with st.spinner("Retrieving and reasoning..."):
                chunks = search_backend_index(user_question, st.session_state.retriever_k)
                if not chunks:
                    st.session_state.history.append({
                        "role": "assistant",
                        "content": "KhÃ´ng tÃ¬m tháº¥y Ä‘oáº¡n ná»™i dung phÃ¹ há»£p trong index hiá»‡n táº¡i.",
                    })
                    st.rerun()

                context = format_backend_context(chunks)
                answer = call_llm(st.session_state.chat_model, user_question, context)

                sources = []
                for item in chunks:
                    meta = item["meta"]
                    source = meta.get("source_filename") or meta.get("filename") or meta.get("doc_id", "unknown")
                    snippet = meta.get("text") or ""
                    breadcrumbs = meta.get("breadcrumbs") or meta.get("section_heading")
                    sources.append(
                        {
                            "source": source,
                            "page": breadcrumbs,
                            "snippet": snippet[:400] + ("..." if snippet and len(snippet) > 400 else ""),
                        }
                    )

                st.session_state.history.append(
                    {
                        "role": "assistant",
                        "content": answer,
                        "sources": sources,
                    }
                )
        except Exception as exc:
            st.session_state.history.append({"role": "assistant", "content": f"ÄÃ£ xáº£y ra lá»—i: {exc}"})

        st.rerun()


def render_sidebar() -> None:
    render_settings_body()
    st.divider()
    render_sidebar_quick_actions()


if __name__ == "__main__":
    main()

